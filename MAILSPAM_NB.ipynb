{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Spam Detection: Combine CSVs & Preprocess\n",
        "# =========================================\n",
        "\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "import string\n",
        "\n",
        "# =========================================\n",
        "# Step 2: Extract ZIP file\n",
        "# =========================================\n",
        "zip_path = \"/content/archive (2).zip\"  # path to your uploaded zip\n",
        "extract_folder = \"/content/spam_data\"  # folder to extract CSVs\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(\"ZIP extracted successfully!\")\n",
        "\n",
        "# =========================================\n",
        "# Step 3: Read and combine all CSVs\n",
        "# =========================================\n",
        "all_files = glob.glob(extract_folder + \"/*.csv\")\n",
        "df_list = []\n",
        "\n",
        "for file in all_files:\n",
        "    df = pd.read_csv(file, encoding='latin-1')\n",
        "\n",
        "    # Standardize text column\n",
        "    if 'text_combined' in df.columns:\n",
        "        df['text'] = df['text_combined']\n",
        "    elif 'subject' in df.columns and 'body' in df.columns:\n",
        "        df['text'] = df['subject'].fillna('') + ' ' + df['body'].fillna('')\n",
        "    elif 'body' in df.columns:\n",
        "        df['text'] = df['body']\n",
        "    else:\n",
        "        continue  # skip if no usable text column\n",
        "\n",
        "    # Standardize label column\n",
        "    if 'label' in df.columns:\n",
        "        # Convert all labels to string, strip spaces, lowercase\n",
        "        df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
        "        # Map numeric or other variants to 'spam'/'ham'\n",
        "        df['label'] = df['label'].replace({\n",
        "            '1': 'spam', '0': 'ham',\n",
        "            's': 'spam', 'h': 'ham'\n",
        "        })\n",
        "    else:\n",
        "        continue  # skip if no label\n",
        "\n",
        "    # Keep only rows with 'spam' or 'ham'\n",
        "    df = df[df['label'].isin(['spam', 'ham'])]\n",
        "    df = df[['label', 'text']]  # keep only relevant columns\n",
        "    df_list.append(df)\n",
        "\n",
        "# Combine all CSVs\n",
        "data = pd.concat(df_list, ignore_index=True)\n",
        "print(\"Combined dataset shape after filtering:\", data.shape)\n",
        "\n",
        "# =========================================\n",
        "# Step 4: Drop missing text\n",
        "# =========================================\n",
        "data = data.dropna(subset=['text']).copy()\n",
        "print(\"After dropping missing text:\", data.shape)\n",
        "\n",
        "# =========================================\n",
        "# Step 5: Text preprocessing\n",
        "# =========================================\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# =========================================\n",
        "# Step 6: Map labels to numeric for ML\n",
        "# =========================================\n",
        "data['label'] = data['label'].map({'ham':0, 'spam':1})\n",
        "\n",
        "# =========================================\n",
        "# Step 7: Check sample\n",
        "# =========================================\n",
        "print(\"Sample preprocessed data:\")\n",
        "print(data.head())\n",
        "print(\"\\nTotal messages in dataset:\", len(data))\n",
        "print(\"Label distribution:\\n\", data['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeWhGU9CDyCF",
        "outputId": "661bfa0e-c88e-48ce-df59-047a78e25489"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP extracted successfully!\n",
            "Combined dataset shape after filtering: (164972, 2)\n",
            "After dropping missing text: (164972, 2)\n",
            "Sample preprocessed data:\n",
            "   label                                               text\n",
            "0      1  dont delete this message  folder internal data...\n",
            "1      1  verify your account business with  \\t\\t\\t\\t\\t\\...\n",
            "2      1  helpdesk mailbox alert your two incoming mails...\n",
            "3      1  itservice help desk password will expire in 3 ...\n",
            "4      1  final usaa reminder  update your account now t...\n",
            "\n",
            "Total messages in dataset: 164972\n",
            "Label distribution:\n",
            " label\n",
            "1    85782\n",
            "0    79190\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# Step 8: Split dataset into train/test\n",
        "# =========================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", len(X_train))\n",
        "print(\"Testing samples:\", len(X_test))\n",
        "\n",
        "# =========================================\n",
        "# Step 9: TF-IDF Vectorization\n",
        "# =========================================\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # top 5000 words\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# =========================================\n",
        "# Step 10: Train Multinomial Naive Bayes\n",
        "# =========================================\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# =========================================\n",
        "# Step 11: Evaluate Model\n",
        "# =========================================\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# =========================================\n",
        "# Step 12: Predict New Messages\n",
        "# =========================================\n",
        "new_messages = [\n",
        "    \"Congratulations! You've won a free ticket. Call now!\",\n",
        "    \"Hey, are we meeting today for lunch?\"\n",
        "]\n",
        "\n",
        "# Preprocess\n",
        "new_messages_clean = [text.lower().translate(str.maketrans('', '', string.punctuation)) for text in new_messages]\n",
        "new_messages_tfidf = vectorizer.transform(new_messages_clean)\n",
        "predictions = nb_model.predict(new_messages_tfidf)\n",
        "\n",
        "for msg, pred in zip(new_messages, predictions):\n",
        "    print(f\"\\nMessage: {msg}\")\n",
        "    print(\"Predicted:\", \"Spam\" if pred==1 else \"Not Spam\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-biCtzzFUvo",
        "outputId": "9a5e7193-159a-4603-e550-1d438a5defc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 131977\n",
            "Testing samples: 32995\n",
            "Accuracy: 0.9613577814820428\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96     15835\n",
            "           1       0.98      0.95      0.96     17160\n",
            "\n",
            "    accuracy                           0.96     32995\n",
            "   macro avg       0.96      0.96      0.96     32995\n",
            "weighted avg       0.96      0.96      0.96     32995\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15472   363]\n",
            " [  912 16248]]\n",
            "\n",
            "Message: Congratulations! You've won a free ticket. Call now!\n",
            "Predicted: Spam\n",
            "\n",
            "Message: Hey, are we meeting today for lunch?\n",
            "Predicted: Not Spam\n"
          ]
        }
      ]
    }
  ]
}